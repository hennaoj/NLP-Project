{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28bb9511",
   "metadata": {},
   "source": [
    "Task 7. Instead of cheking the Wu & Palmer similarity like in Task 5, the compatibility between the node noun and the collocates (of the adjective) is checked using domain information. If at least one of the collocates belong to the same domain as the noun, they are compatible and the phrase is classified as not a metaphor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40b01714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.bnc import BNCCorpusReader\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "226ec1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only using folders A-C for now\n",
    "reader = BNCCorpusReader(root='BNC/Texts/', fileids=r'[A-C]/\\w*/\\w*\\.xml')\n",
    "sents = reader.tagged_sents()\n",
    "words = reader.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b4b7b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some preprocessing (removing special characters and numbers from the list of words and changing all the words into lower case)\n",
    "#runtime would be about 1.5 hours if using the whole corpus, though probably (and hopefully) shorter on a better computer\n",
    "\n",
    "wordlist = []\n",
    "special_chars = ['(',')',',','\"','.','!','?','-','\\'','‘','’','—',':']\n",
    "for w in words:\n",
    "    if w not in special_chars and not w.isnumeric():\n",
    "        wordlist.append(w.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e08483de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37525438"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "926eca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pair(sentence):\n",
    "    '''\n",
    "    Finds adjective/adverb-noun part-of-speech in a given sentence using nltk part-of-speech tagging. \n",
    "    Returns only the first occurence of such pair in a sentence.\n",
    "    '''\n",
    "    pair = []\n",
    "    tagged_words = nltk.pos_tag(sentence.split())\n",
    "    adjectives = ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS']\n",
    "    nouns = ['NN', 'NNS', 'NNPS', 'NNP']\n",
    "    for i in range(len(tagged_words) - 1):\n",
    "        word1_category = tagged_words[i][1]\n",
    "        word2_category = tagged_words[i + 1][1]\n",
    "        if word1_category in adjectives and word2_category in nouns:\n",
    "            pair = [tagged_words[i][0].lower(), re.sub('\\W+','', tagged_words[i + 1][0]).lower()]\n",
    "            return pair\n",
    "    return pair\n",
    "\n",
    "def check_senses(pair):\n",
    "    '''\n",
    "    Given an adjective/adverb-noun pair checks that the adjective/adverb has more than one sense and the noun has an\n",
    "    entry in WordNet.\n",
    "    '''\n",
    "    adj = pair[0]\n",
    "    noun = pair[1]\n",
    "    if len(wn.synsets(adj)) == 1:\n",
    "        print('Adjective has only one sense!')\n",
    "        return False\n",
    "    elif len(wn.synsets(noun)) == 0:\n",
    "        print('Noun has no entry in WordNet!')\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def find_words_near(node):\n",
    "    '''\n",
    "    Finds nouns appearing next to a given node word by checking each sentence of the corpus individually.\n",
    "    '''\n",
    "    print('Looking for words appearing next to', node)\n",
    "    words_near = []\n",
    "    for sentence in sents:\n",
    "        for i in range(len(sentence)):\n",
    "            if sentence[i][0] == node:\n",
    "                indexesToTry = [i - 1, i + 1]\n",
    "                for index in indexesToTry:\n",
    "                    if index >= 0 and index < len(sentence):\n",
    "                        if sentence[index][1] and sentence[index][1] == 'SUBST':\n",
    "                            words_near.append(sentence[index][0].lower())\n",
    "    return words_near\n",
    "\n",
    "def find_collocates(node, words_near):\n",
    "    '''\n",
    "    Finds all unique collocate nouns from a list of nouns that appear near the node word. A noun is considered a collocate\n",
    "    when its mutual information to the node is greater or equal to 3. Only considers nouns that appear at least twice\n",
    "    near the node word.\n",
    "    '''\n",
    "    print('Determining the collocates of', node)\n",
    "    collocates = []\n",
    "    checked = []\n",
    "    freq_node = wordlist.count(node)\n",
    "    for word in words_near:\n",
    "        if word not in checked:\n",
    "            checked.append(word)\n",
    "            freq_near = words_near.count(word)\n",
    "            if freq_near >= 2:\n",
    "                freq_collocate = wordlist.count(word)\n",
    "                mutual_information = calculate_mutual_information(freq_node, freq_collocate, freq_near)\n",
    "                if mutual_information >= 3:\n",
    "                    if (word, mutual_information) not in collocates:\n",
    "                        collocates.append((word, mutual_information))\n",
    "    return collocates\n",
    "\n",
    "def calculate_mutual_information(freq_node, freq_collocate, freq_near):\n",
    "    '''\n",
    "    Calculates the mutual information between a node and a possible collocate using the expression (2) in\n",
    "    the article https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0062343 by Neuman et al (2013).\n",
    "    '''\n",
    "    corpus_size = len(wordlist)\n",
    "    span = 3 #maybe?\n",
    "    mutual_information = np.log10((freq_near * corpus_size)/(freq_node * freq_collocate * span))/np.log10(2)\n",
    "    return mutual_information\n",
    "\n",
    "with open('WordNet2/Words.cat', 'r') as file:\n",
    "    data = file.readlines()\n",
    "    nouns = data[25040:94946]\n",
    "    \n",
    "tidy_nouns = []\n",
    "for noun in nouns:\n",
    "    noun = noun.replace(' (1)', '')\n",
    "    noun = noun.replace('\\t', '')\n",
    "    noun = noun.replace('\\n', '')\n",
    "    noun = noun.lower()\n",
    "    tidy_nouns.append(noun)\n",
    "\n",
    "def find_classes(collocates):\n",
    "    '''\n",
    "    Classifies a list of nouns using WordStat. Returns a list of nouns (and their mutual information also given in\n",
    "    the input list) that belong to a concrete class. The list is sorted by the mutual information value in ascending order.\n",
    "    '''\n",
    "    Lem = WordNetLemmatizer()\n",
    "\n",
    "    current_class = ''\n",
    "    classified_nouns = []\n",
    "    \n",
    "    #there should be 13 concrete classes out of the 25 classes, now there is 12 and some might be incorrect :D\n",
    "    concrete_classes = ['animal', 'artifact', 'body', 'event', 'food', 'location', 'object', 'person', 'possession',\n",
    "                       'plant', 'shape', 'substance']\n",
    "\n",
    "    for target in collocates:\n",
    "        if tidy_nouns.count(target[0]) > 0:\n",
    "            for noun in tidy_nouns:\n",
    "                #changing the noun class (e.g. when NOUN.FOOD is encountered, class is FOOD until the next noun\n",
    "                #class is encountered)\n",
    "                if 'noun.' in noun:\n",
    "                    noun_split = noun.split('.')\n",
    "                    current_class = noun_split[1]\n",
    "                elif noun == Lem.lemmatize(target[0]) and current_class in concrete_classes:\n",
    "                    #only adding nouns that are not already in the list (some might be in more than one conrete class)\n",
    "                    if (noun, target[1]) not in classified_nouns:\n",
    "                        classified_nouns.append((noun, target[1]))\n",
    "    #sorting by the mutual information value\n",
    "    classified_nouns = sorted(classified_nouns, key=itemgetter(1))\n",
    "    return classified_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14ac4ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Because the WordNet domain categorising uses version 2.0 (or 1.6) of WordNet and the current nltk version is 3.0,\n",
    "#we need to download WordNet version 2 and use the noun.data file in the dict folder to get the correct offset ids for synsets.\n",
    "\n",
    "with open('WordNet2.0/dict/noun.dat', 'r') as file:\n",
    "    wn_data = file.readlines()[29:79719]\n",
    "\n",
    "def find_offsets(node):\n",
    "    '''\n",
    "    Returns all possible WordNet offset ids for a given noun.\n",
    "    '''\n",
    "    offsets = []\n",
    "    for line in wn_data:\n",
    "        line = line.split()\n",
    "        offset = line[0]\n",
    "        pos = line[2]\n",
    "        word = line[4]\n",
    "        if word == node:\n",
    "            offsets.append(offset)\n",
    "    return offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "143bda8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Domains can be downloaded from https://wndomains.fbk.eu/download.html\n",
    "\n",
    "\n",
    "with open('Domains/domains_2', 'r') as file: #domains_2 = wn-domains-3.2-20070223\n",
    "    domain_data = file.readlines()\n",
    "\n",
    "def find_domains(target_offset):\n",
    "    '''\n",
    "    Returns the domains matching a given WordNet offset id.\n",
    "    '''\n",
    "    found_domains = []\n",
    "    for line in domain_data:\n",
    "        line = line.split('-')\n",
    "        offset = line[0].lstrip(\"0\")\n",
    "        domains = line[1][2:-1]\n",
    "        if offset == target_offset:\n",
    "            domains = domains.split(' ')\n",
    "            for domain in domains:\n",
    "                found_domains.append(domain)\n",
    "    return found_domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cc7c2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_a_metaphor(sentence):\n",
    "    '''\n",
    "    Determines whether a sentence includes a type III metaphor (adjective/adverb-noun metaphor) by going through a set of steps.\n",
    "    '''\n",
    "    pair = find_pair(sentence)\n",
    "    if not pair:\n",
    "        print('No pair!')\n",
    "        return\n",
    "    print('Pair found: ', pair)\n",
    "    \n",
    "    check = check_senses(pair)\n",
    "    if not check:\n",
    "        print('Sense check failed!')\n",
    "        return\n",
    "    print('Sense check ok!')\n",
    "    \n",
    "    adjective = pair[0]\n",
    "    noun = pair[1]\n",
    "    \n",
    "    words_near = find_words_near(adjective)\n",
    "    print('Found', len(words_near), 'nouns near', adjective)\n",
    "    \n",
    "    collocates = find_collocates(adjective, words_near)\n",
    "    print('Found', len(collocates), 'unique collocates')\n",
    "    \n",
    "    if not collocates:\n",
    "        print('Undetermined, not enough collocates')\n",
    "        return\n",
    "\n",
    "    classified_nouns = find_classes(collocates)\n",
    "    print(len(classified_nouns), 'collocate words in concrete classes')\n",
    "    \n",
    "    top_three = classified_nouns[-4:-1]\n",
    "    print('The top three collocates are', top_three)\n",
    "    \n",
    "    if not classified_nouns:\n",
    "        return 'Is a metaphor!'\n",
    "\n",
    "    node_offsets = find_offsets(noun)\n",
    "    for offset in node_offsets:\n",
    "        node_domains = find_domains(offset.lstrip(\"0\"))\n",
    "    \n",
    "    collocate_domains = []\n",
    "    for collocate in top_three:\n",
    "        offsets = find_offsets(collocate[0])\n",
    "        for offset in offsets:\n",
    "            collocate_domains.append(find_domains(offset.lstrip(\"0\")))\n",
    "            \n",
    "    collocate_domains = [domain for domain in collocate_domains]\n",
    "            \n",
    "    for domain in node_domains:\n",
    "        if any(domain in sublist for sublist in collocate_domains):\n",
    "            print('At least one of the top three collocates and the node noun belong to the domain', domain)\n",
    "            return 'Not a metaphor!'\n",
    "    \n",
    "    return 'Is a metaphor!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c59f27ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair found:  ['dramatic', 'person']\n",
      "Sense check ok!\n",
      "Looking for words appearing next to dramatic\n",
      "Found 1299 nouns near dramatic\n",
      "Determining the collocates of dramatic\n",
      "Found 118 unique collocates\n",
      "43 collocate words in concrete classes\n",
      "The top three collocates are [('transformation', 6.50273567357617), ('skyline', 6.701349709312811), ('scenery', 7.781378090728456)]\n",
      "At least one of the top three collocates and the node noun belong to the domain grammar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Not a metaphor!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_a_metaphor('She is such a dramatic person!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50619612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair found:  ['green', 'thumb']\n",
      "Sense check ok!\n",
      "Looking for words appearing next to green\n",
      "Found 2329 nouns near green\n",
      "Determining the collocates of green\n",
      "Found 172 unique collocates\n",
      "113 collocate words in concrete classes\n",
      "The top three collocates are [('blob', 7.5282568204702836), ('sward', 8.100163168365993), ('malachite', 9.287248720966488)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Is a metaphor!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_a_metaphor('He has a green thumb.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3a6e947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair found:  ['curious', 'cat']\n",
      "Sense check ok!\n",
      "Looking for words appearing next to curious\n",
      "Found 551 nouns near curious\n",
      "Determining the collocates of curious\n",
      "Found 38 unique collocates\n",
      "16 collocate words in concrete classes\n",
      "The top three collocates are [('blend', 5.802967630654725), ('traveller', 5.996954542556096), ('mixture', 6.8080318390906145)]\n",
      "At least one of the top three collocates and the node noun belong to the domain factotum\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Not a metaphor!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_a_metaphor('I have a curious cat who likes to get into trouble.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bd382bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair found:  ['dead', 'center']\n",
      "Sense check ok!\n",
      "Looking for words appearing next to dead\n",
      "Found 1436 nouns near dead\n",
      "Determining the collocates of dead\n",
      "Found 68 unique collocates\n",
      "41 collocate words in concrete classes\n",
      "The top three collocates are [('nettle', 6.918619434687318), ('magpie', 8.097290478594447), ('aspidistra', 8.853524406465432)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Is a metaphor!'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_a_metaphor('They stood in the dead center of the room.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd33405b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair found:  ['cold', 'heart']\n",
      "Sense check ok!\n",
      "Looking for words appearing next to cold\n",
      "Found 1861 nouns near cold\n",
      "Determining the collocates of cold\n",
      "Found 86 unique collocates\n",
      "48 collocate words in concrete classes\n",
      "The top three collocates are [('sweat', 6.65484878398373), ('chisel', 7.3439196880456254), ('lino', 7.532364777458736)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Is a metaphor!'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_a_metaphor('He has a cold heart.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2e22d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
